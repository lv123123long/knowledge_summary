# vLLM概述
vLLM 框架是一个高效的大语言模型推理和部署服务系统，具备以下特性
高效的内存管理：通过 PagedAttention 算法，vLLM 实现了对 KV 缓存的高效管理，减少了内存浪费，优化了模型的运行效率。
高吞吐量：vLLM 支持异步处理和连续批处理请求，显著提高了模型推理的吞吐量，加速了文本生成和处理速度。
易用性：vLLM 与 HuggingFace 模型无缝集成，支持多种流行的大型语言模型，简化了模型部署和推理的过程。兼容 OpenAI 的 API 服务器。
分布式推理：框架支持在多 GPU 环境中进行分布式推理，通过模型并行策略和高效的数据通信，提升了处理大型模型的能力。
开源共享：vLLM 由于其开源的属性，拥有活跃的社区支持，这也便于开发者贡献和改进，共同推动技术发展。


## 参考

[[大模型]Qwen2-7B-Instruct vLLM 部署调用_vllm qwen2-CSDN博客](https://blog.csdn.net/FL1623863129/article/details/139693141)